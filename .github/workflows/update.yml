name: scrape-and-update

on:
  schedule:
    - cron: '0 8 * * *'        # todos los días a las 08:00 UTC
  workflow_dispatch:           # ejecución manual si la necesitas

permissions:
  contents: write              # permite hacer push al repo

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    steps:
    # 1. Clonar el repo
    - name: Checkout
      uses: actions/checkout@v4

    # 2. Preparar Python (v. estable con rueda de lxml)
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    # 3. Instalar dependencias de requirements.txt
    - name: Instalación de dependencias
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # 4. Ejecutar el orquestador de scrapers
    - name: Ejecutar scrapers
      run: python main.py

    # 5. Sincronizar rama local con la punta remota (evita non-fast-forward)
    - name: Sincronizar con remoto
      run: |
        git fetch origin main
        git rebase origin/main

    # 6. Commit + push de los CSV modificados
    - name: Commit cambios CSV
      run: |
        git config user.name  "gh-actions"
        git config user.email "bot@example.com"

        # Añade los CSV generados/actualizados
        git add actualizaciones_expedientes*.csv

        # Si hay cambios, comitea y empuja; si otro push apareció entre medias,
        # --force-with-lease abortará en lugar de sobrescribirlo.
        git diff --cached --quiet || \
          (git commit -m "auto: datos $(date -u +'%F %T') [skip ci]" && \
           git push --force-with-lease)
